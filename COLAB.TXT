# Cell 1: Install dependencies
!pip install -q transformers sentence-transformers faiss-cpu pypdf langchain langchain_community langchain_core torch flask flask-cors pyngrok ctransformers

# Cell 2: Imports and Setup
import os
import pickle
import faiss
import torch
import numpy as np
from flask import Flask, request, jsonify, make_response
from flask_cors import CORS
from pypdf import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.embeddings import HuggingFaceEmbeddings
from ctransformers import AutoModelForCausalLM
from pyngrok import ngrok, conf

# Cell 3: Initialize Mistral 7B Model - Fixed simplified version
print("Loading Mistral 7B Instruct via ctransformers (direct approach)...")

from ctransformers import AutoModelForCausalLM

# Load the model with direct parameters
model = AutoModelForCausalLM.from_pretrained(
    "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
    model_file="mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    gpu_layers=40,  # Adjust based on your GPU
    context_length=4096  # Set max context length
)

# Direct function for text generation with Mistral
def generate_mistral_response(prompt, max_tokens=512):
    # Format Mistral prompt
    formatted_prompt = f"<s>[INST] {prompt} [/INST]"

    # Generate response
    response = model(
        formatted_prompt,
        max_new_tokens=max_tokens,
        temperature=0.1,
        repetition_penalty=1.15,
        top_p=0.95,
        top_k=40,
        stop=["</s>", "[INST]"]  # Stop at end of generation or new instruction
    )

    # Extract only the generated text (remove the prompt)
    generated_text = response.replace(formatted_prompt, "").strip()
    return generated_text

# Simple wrapper class for compatibility with previous code
class MistralGenerator:
    def __init__(self, max_tokens=512):
        self.max_tokens = max_tokens

    def invoke(self, prompt):
        return generate_mistral_response(prompt, self.max_tokens)

# Create our LLM instance
llm = MistralGenerator(max_tokens=512)

# Test the model
try:
    test_prompt = "What is 2+2?"
    test_result = llm.invoke(test_prompt)
    print("LLM Test Result:", test_result)
    if test_result:
        print("✅ Mistral 7B is working properly!")
    else:
        print("⚠️ LLM returned empty result")
except Exception as e:
    print("❌ LLM Error:", e)
    import traceback
    traceback.print_exc()

# Cell 4: Initialize Embeddings Model
# Use BGE embeddings for better retrieval quality
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-en-v1.5",
    model_kwargs={'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')},
    encode_kwargs={'normalize_embeddings': True}
)

# Cell 5: Advanced Prompt Template
quality_prompt = ChatPromptTemplate.from_template("""<|system|>
You are an expert academic assistant. Provide comprehensive answers with examples and evidence from the provided documents. If the information is not in the documents, clearly state that you don't know.
</|system|>

<|user|>
Analyze the context and provide:
1. Comprehensive answer
2. Relevant examples
3. Key evidence
4. Practical applications

Context: {context}
Question: {input}
</|user|>""")

# Cell 6: Vector Store Management
class VectorStore:
    def __init__(self):
        self.documents = []
        self.vector_store = None
        self.initialize_store()

    def initialize_store(self):
        self.vector_store = FAISS.from_texts(["Initialize"], embeddings)

    def add_documents(self, chunks):
        if not self.vector_store:
            self.initialize_store()
        self.vector_store.add_documents(chunks)
        self.documents.extend(chunks)

    def save(self):
        if not self.vector_store:
            print("No vector store to save")
            return
        faiss.write_index(self.vector_store.index, "vector_store.index")
        with open("documents.pkl", "wb") as f:
            pickle.dump(self.documents, f)
        print("Vector store saved successfully")

    def load(self):
        try:
            if os.path.exists("vector_store.index") and os.path.exists("documents.pkl"):
                if not self.vector_store:
                    self.initialize_store()
                self.vector_store.index = faiss.read_index("vector_store.index")
                with open("documents.pkl", "rb") as f:
                    self.documents = pickle.load(f)
                print("Loaded existing vector store")
                return True
            else:
                print("No existing vector store found, starting fresh")
                return False
        except Exception as e:
            print(f"Error loading vector store: {str(e)}")
            return False

# Initialize the vector store
vector_db = VectorStore()
vector_db.load()  # Try to load existing vector store

# Cell 7: Enhanced PDF Processing
def process_pdf(file_path):
    text = ""
    try:
        with open(file_path, "rb") as f:
            reader = PdfReader(f)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:  # Check if text extraction was successful
                    text += page_text + "\n"

        if not text.strip():
            return "⚠️ No text extracted from the PDF"

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # Smaller chunks for better retrieval
            chunk_overlap=100,
            separators=["\n\n", "\n", "\. ", "!", "?", ", "]
        )
        chunks = text_splitter.create_documents([text])

        if not chunks:
            return "⚠️ No chunks created from the PDF"

        # Add metadata for source tracking - use filename as source
        for chunk in chunks:
            if not hasattr(chunk, 'metadata'):
                chunk.metadata = {}
            chunk.metadata["source"] = os.path.basename(file_path)

        vector_db.add_documents(chunks)
        vector_db.save()
        return f"✅ Processed {len(chunks)} chunks from {os.path.basename(file_path)}"

    except Exception as e:
        return f"❌ Error processing PDF: {str(e)}"
# Cell 8: Query System with Phi-3 Mini
def ask_question(question):
    if not vector_db.documents or not vector_db.vector_store:
        return {"answer": "⚠️ Please upload PDFs first!", "sources": []}

    try:
        print(f"Processing question: {question}")
        print(f"Vector DB has {len(vector_db.documents)} documents")

        # Retrieve relevant documents from vector store
        retrieved_docs = []
        sources = []

        try:
            # Use FAISS for retrieval
            retriever = vector_db.vector_store.as_retriever(search_kwargs={"k": 5})
            from langchain.schema import Document
            try:
                # Try using the updated invoke method
                docs = retriever.invoke(question)
                retrieved_docs = docs.get("documents", [])
            except:
                # Fall back to legacy get_relevant_documents method
                retrieved_docs = retriever.get_relevant_documents(question)

            # Extract source information
            for i, doc in enumerate(retrieved_docs):
                excerpt = doc.page_content[:150] + "..." if len(doc.page_content) > 150 else doc.page_content
                title = doc.metadata.get("source", f"Document {i}")
                sources.append({"title": title, "excerpt": excerpt})

            print(f"Successfully retrieved {len(retrieved_docs)} documents")

        except Exception as retriever_error:
            print(f"Error with FAISS retrieval: {str(retriever_error)}")
            import traceback
            traceback.print_exc()

            # Fallback to simple text similarity search
            try:
                print("Trying simple text similarity as fallback...")

                # Find documents that contain similar words to the question
                relevant_docs = []
                question_words = set(question.lower().split())

                for doc in vector_db.documents:
                    doc_words = set(doc.page_content.lower().split())
                    overlap = len(question_words.intersection(doc_words))
                    if overlap > 0:
                        relevant_docs.append((doc, overlap))

                # Sort by overlap and take top 5
                relevant_docs.sort(key=lambda x: x[1], reverse=True)
                retrieved_docs = [doc for doc, _ in relevant_docs[:5]]

                # Extract source info
                sources = []
                for i, (doc, _) in enumerate(relevant_docs[:5]):
                    excerpt = doc.page_content[:150] + "..." if len(doc.page_content) > 150 else doc.page_content
                    title = doc.metadata.get("source", f"Document {i}")
                    sources.append({"title": title, "excerpt": excerpt})

                print(f"Fallback found {len(retrieved_docs)} documents")

            except Exception as fallback_error:
                print(f"Fallback retrieval also failed: {str(fallback_error)}")
                retrieved_docs = []
                sources = []

        # If we have no documents, generate a general answer
        if not retrieved_docs:
            print("No relevant documents found, generating general answer...")
            try:
                answer = llm.invoke(question)
                return {"answer": answer, "sources": []}
            except Exception as direct_error:
                print(f"Error generating direct answer: {str(direct_error)}")
                return {"answer": "I couldn't find relevant information to answer your question.", "sources": []}

        # Generate answer using the retrieved documents
        print(f"Generating answer with {len(retrieved_docs)} documents...")

        try:
            # Build context from retrieved documents
            context = "\n\n".join([doc.page_content for doc in retrieved_docs])

            # Create prompt for Mistral format
            mistral_prompt = f"""You are a helpful AI assistant. Answer questions based only on the provided context. Be clear, concise, and well-organized.

Based on the following information, please answer the question.

Context:
{context}

Question: {question}"""

            # Generate answer with Mistral
            answer = llm.invoke(mistral_prompt)
            print("Successfully generated answer")

            return {"answer": answer, "sources": sources}

        except Exception as answer_error:
            print(f"Error generating answer: {str(answer_error)}")
            import traceback
            traceback.print_exc()
            return {"answer": f"Error generating answer: {str(answer_error)}", "sources": sources}

    except Exception as e:
        print(f"General error in ask_question: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"answer": f"Error: {str(e)}", "sources": []}

# Cell 9: Flask API Setup with ngrok
app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}}, supports_credentials=False)

# Add CORS headers
@app.after_request
def after_request(response):
    response.headers.add('Access-Control-Allow-Origin', '*')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')
    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')
    return response

@app.route('/upload', methods=['POST'])
def upload_endpoint():
    if 'files' not in request.files:
        return jsonify({'error': 'No files provided'}), 400

    files = request.files.getlist('files')
    results = []

    for file in files:
        if file.filename.endswith('.pdf'):
            # Save the file temporarily
            temp_path = f"/tmp/{file.filename}"
            file.save(temp_path)

            # Process the PDF
            result = process_pdf(temp_path)
            results.append(result)

            # Clean up
            try:
                os.remove(temp_path)
            except:
                pass

    if not results:
        return jsonify({'message': 'No PDFs were uploaded.'}), 400

    return jsonify({'message': '\n'.join(results)}), 200

@app.route('/ask', methods=['POST'])
def ask_endpoint():
    data = request.json
    print("\n---- ASK ENDPOINT ----")
    print("Request data:", data)

    if not data or 'question' not in data:
        print("Error: No question provided")
        return jsonify({'error': 'No question provided'}), 400

    question = data['question']
    print("Processing question:", question)

    try:
        # Test LLM availability before processing
        try:
            test_prompt = "<|user|>test</|user|>"
            test_result = llm.invoke(test_prompt)
            print("✅ Phi-3 Mini test successful")
        except Exception as llm_test_error:
            print(f"❌ LLM test failed: {str(llm_test_error)}")
            return jsonify({'answer': "Error: Could not connect to the Phi-3 Mini model. Please try again later.", 'sources': []}), 200

        result = ask_question(question)
        print("Got result from ask_question function:")
        print("- Answer length:", len(result.get('answer', '')) if result.get('answer') else 'No answer')
        print("- Sources count:", len(result.get('sources', [])))
        print("- Answer preview:", result.get('answer', '')[:100], "..." if result.get('answer') and len(result.get('answer', '')) > 100 else "")

        return jsonify(result), 200
    except Exception as e:
        print("Error in ask_endpoint:", str(e))
        import traceback
        traceback.print_exc()
        return jsonify({'answer': f'Error processing question: {str(e)}', 'sources': []}), 200  # Return 200 so frontend displays the error

@app.route('/ping', methods=['GET', 'OPTIONS'])
def ping():
    if request.method == 'OPTIONS':
        return make_response('', 204)
    return jsonify({'status': 'pong', 'message': 'API is working with Phi-3 Mini!'}), 200

# Cell 10: Configure ngrok and run Flask app
# You need to sign up for a free account at ngrok.com and get your authtoken
NGROK_AUTH_TOKEN = "2biVQ2NBtgxrL8PhGvtGIfYpB79_3ANkSuGqSywhKiQKKQnxK"  # Replace with your actual authtoken

def run_flask_app_with_ngrok():
    # Set ngrok auth token
    ngrok.set_auth_token(NGROK_AUTH_TOKEN)

    port = 5000
    # Create a tunnel to the Flask app
    public_url = ngrok.connect(port).public_url

    print(f"\n{'='*70}")
    print(f"✅ NGROK Public URL: {public_url}")
    print(f"⚠️ IMPORTANT: Update API_BASE_URL in your Next.js app/api/client.ts file to:")
    print(f"const API_BASE_URL = '{public_url}';")
    print(f"{'='*70}")

    # Display a clickable link to test the API
    print(f"\nTest your API: {public_url}/ping")

    # Run the Flask app
    app.run(port=port)

# Cell 11: Test the Phi-3 Mini model
print("\nTesting Phi-3 Mini...")
try:
    test_prompt = "<|user|>What is 2+2?</|user|>"
    test_result = llm.invoke(test_prompt)
    print("Phi-3 Mini Test Result:", test_result)
    print("✅ Phi-3 Mini is working correctly!")
except Exception as e:
    print("❌ Phi-3 Mini error:", e)

# Cell 11: Main execution point - run this cell to start the API
if __name__ == "__main__":
    # Verify we have everything set up
    print("System status:")
    print(f"- Vector store initialized: {vector_db.vector_store is not None}")
    print(f"- Documents loaded: {len(vector_db.documents)}")
    print(f"- Embedding model: {embeddings.model_name}")
    print(f"- LLM model: Mistral 7B Instruct (Q4 quantized)")

    print("\nStarting Flask API with ngrok tunneling...")
    run_flask_app_with_ngrok()